{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hello World! Inference - Bring Your Own Pickle File 'Model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: dill in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (0.3.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install dill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import dill as pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create our Custom Hello World 'Model'\n",
    "Here we create a python class called Model and inside it we have a method called predict that takes an integer and returns \"Hello World!\" that many times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def predict(x): \n",
    "        if isinstance(x, (int)):  # Check if x is a number\n",
    "            sentence = \"Hello, World! \" * x  # Repeat \"Hello, World!\" x times\n",
    "            return sentence  # Print the sentence\n",
    "        else:\n",
    "            print(\"Error: input must be an integer\")  # Display an error message if x is not a number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pickle the Model. \n",
    "Now we want to serialize our custom 'Model'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('hello-world-model.pkl', 'wb') as f:\n",
    "    pickle.dump(Model, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the model and test\n",
    "Now we can deserialize the model and test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('hello-world-model.pkl', 'rb') as f:\n",
    "    model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello, World! Hello, World! '"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "raw_data = 2\n",
    "\n",
    "model.predict(raw_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Package our Model to deploy to a SageMaker endpoint\n",
    "SageMaker requires our Model to be tared and gzipped. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello-world-model.pkl\n"
     ]
    }
   ],
   "source": [
    "! tar -czvf hello_world_model.tar.gz hello-world-model.pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload our Model to S3\n",
    "Now we can upload our Model pickeled (serialized) Model to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: ./hello_world_model.tar.gz to s3://sagemaker-us-east-1-171503325295/DEMO-hello-world/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "\n",
    "bucket = sagemaker.Session().default_bucket()\n",
    "prefix = 'DEMO-hello-world'\n",
    "\n",
    "    \n",
    "! aws s3 cp hello_world_model.tar.gz s3://$bucket/$prefix/model.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and Push our container to ECR\n",
    "We have our custom Model that is now in S3. All we need now is a container that implemenets the hosting requirements and inference logic.\n",
    "An important file to look at is the predictor.py here we coded the logic to deserialize the Model and make a inference from it. SageMaker fetched our Model from S3 and placed it in /opt/ml/model/. Take a look at the get_model() method which uses the code above to load the model from the SageMaker model path. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    def get_model(cls):\n",
      "        \"\"\"Get the model object for this instance, loading it if it's not already loaded.\"\"\"\n",
      "        if cls.model == None:\n",
      "            with open(os.path.join(model_path, 'hello-world-model.pkl'), 'rb') as f:\n",
      "                cls.model = pickle.load(f)\n",
      "        return cls.model\n"
     ]
    }
   ],
   "source": [
    "!sed -n '26,31p' container/Files/predictor.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login Succeeded\n",
      "Sending build context to Docker daemon  18.43kB\n",
      "Step 1/9 : FROM python:3.10-slim-buster\n",
      " ---> 93b9055430ce\n",
      "Step 2/9 : RUN pip install --upgrade pip\n",
      " ---> Using cache\n",
      " ---> 202699474e01\n",
      "Step 3/9 : RUN apt-get update && apt-get install -y     wget     nginx     ca-certificates     && rm -rf /var/lib/apt/lists/*\n",
      " ---> Using cache\n",
      " ---> a8bcacb11fbf\n",
      "Step 4/9 : RUN pip3 install flask gevent gunicorn pandas dill\n",
      " ---> Using cache\n",
      " ---> e2a0855d8700\n",
      "Step 5/9 : ENV PYTHONUNBUFFERED=TRUE\n",
      " ---> Using cache\n",
      " ---> f09967b6f1d0\n",
      "Step 6/9 : ENV PYTHONDONTWRITEBYTECODE=TRUE\n",
      " ---> Using cache\n",
      " ---> 9075a508c39a\n",
      "Step 7/9 : ENV PATH=\"/opt/program:${PATH}\"\n",
      " ---> Using cache\n",
      " ---> a1631373de7b\n",
      "Step 8/9 : COPY Files /opt/program\n",
      " ---> Using cache\n",
      " ---> 229f7565b5d3\n",
      "Step 9/9 : WORKDIR /opt/program\n",
      " ---> Using cache\n",
      " ---> add57f4634f8\n",
      "Successfully built add57f4634f8\n",
      "Successfully tagged sagemaker-hello-world-inference:latest\n",
      "The push refers to repository [171503325295.dkr.ecr.us-east-1.amazonaws.com/sagemaker-hello-world-inference]\n",
      "88f6288dc23d: Preparing\n",
      "38a15446bf8f: Preparing\n",
      "c732334ad941: Preparing\n",
      "2741b8af9f98: Preparing\n",
      "c5321f7f53ff: Preparing\n",
      "df6c1b185b95: Preparing\n",
      "b23fedba7dbd: Preparing\n",
      "ae2d55769c5e: Preparing\n",
      "e2ef8a51359d: Preparing\n",
      "df6c1b185b95: Waiting\n",
      "e2ef8a51359d: Waiting\n",
      "b23fedba7dbd: Waiting\n",
      "ae2d55769c5e: Waiting\n",
      "38a15446bf8f: Layer already exists\n",
      "c732334ad941: Layer already exists\n",
      "2741b8af9f98: Layer already exists\n",
      "c5321f7f53ff: Layer already exists\n",
      "88f6288dc23d: Layer already exists\n",
      "df6c1b185b95: Layer already exists\n",
      "b23fedba7dbd: Layer already exists\n",
      "e2ef8a51359d: Layer already exists\n",
      "ae2d55769c5e: Layer already exists\n",
      "latest: digest: sha256:356bd7ef0dd2040e40612eef653134f2cf8083e24bf71c128a23b1a60bae709f size: 2213\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "# The name of our algorithm\n",
    "algorithm_name=sagemaker-hello-world-inference\n",
    "\n",
    "cd container\n",
    "\n",
    "\n",
    "chmod +x Files/serve\n",
    "\n",
    "account=$(aws sts get-caller-identity --query Account --output text)\n",
    "\n",
    "# Get the region defined in the current configuration (default to us-west-2 if none defined)\n",
    "region=$(aws configure get region)\n",
    "region=${region:-us-east-1}\n",
    "\n",
    "fullname=\"${account}.dkr.ecr.${region}.amazonaws.com/${algorithm_name}:latest\"\n",
    "\n",
    "# If the repository doesn't exist in ECR, create it.\n",
    "aws ecr describe-repositories --repository-names \"${algorithm_name}\" > /dev/null 2>&1\n",
    "\n",
    "if [ $? -ne 0 ]\n",
    "then\n",
    "    aws ecr create-repository --repository-name \"${algorithm_name}\" > /dev/null\n",
    "fi\n",
    "\n",
    "# Get the login command from ECR and execute it directly\n",
    "$(aws ecr get-login --region ${region} --no-include-email)\n",
    "\n",
    "# Build the docker image locally with the image name and then push it to ECR\n",
    "# with the full name.\n",
    "\n",
    "docker build  -t ${algorithm_name} .\n",
    "docker tag ${algorithm_name} ${fullname}\n",
    "\n",
    "docker push ${fullname}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy our Model to an Endpoint\n",
    "Our container has been pushed to ECR and our Model is in S3 now we have everything we need to Deploy to a SageMaker Endpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a SageMaker Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.model import Model\n",
    "from sagemaker.local import LocalSession\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "\n",
    "from sagemaker import get_execution_role\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PredictorRealTime(Predictor):\n",
    "    def __init__(self, endpoint_name, sagemaker_session=None):\n",
    "        super(PredictorRealTime, self).__init__(\n",
    "            endpoint_name, \n",
    "            sagemaker_session, \n",
    "            serializer=JSONSerializer(),\n",
    "            deserializer=JSONDeserializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sagemaker_local_session = LocalSession()\n",
    "sagemaker_session = sagemaker.Session()\n",
    "account = sagemaker_session.boto_session.client('sts').get_caller_identity()['Account']\n",
    "region = sagemaker_session.boto_session.region_name\n",
    "\n",
    "image = '{}.dkr.ecr.{}.amazonaws.com/sagemaker-hello-world-inference:latest'.format(account, region)\n",
    "\n",
    "sagemaker_local_model = Model(\n",
    "                    sagemaker_session= sagemaker_local_session,\n",
    "                    model_data = \"s3://\"+bucket+\"/\"+prefix+\"/model.tar.gz\" , \n",
    "                    image_uri= image,\n",
    "                    role=role,\n",
    "                    predictor_cls=PredictorRealTime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deploy locally to test container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attaching to 5au3s64jny-algo-1-izfv3\n",
      "5au3s64jny-algo-1-izfv3  | Starting the inference server with 8 workers.\n",
      "5au3s64jny-algo-1-izfv3  | [2023-12-28 23:35:56 +0000] [10] [INFO] Starting gunicorn 21.2.0\n",
      "5au3s64jny-algo-1-izfv3  | [2023-12-28 23:35:56 +0000] [10] [INFO] Listening at: unix:/tmp/gunicorn.sock (10)\n",
      "5au3s64jny-algo-1-izfv3  | [2023-12-28 23:35:56 +0000] [10] [INFO] Using worker: gevent\n",
      "5au3s64jny-algo-1-izfv3  | [2023-12-28 23:35:56 +0000] [12] [INFO] Booting worker with pid: 12\n",
      "5au3s64jny-algo-1-izfv3  | [2023-12-28 23:35:56 +0000] [13] [INFO] Booting worker with pid: 13\n",
      "5au3s64jny-algo-1-izfv3  | [2023-12-28 23:35:56 +0000] [14] [INFO] Booting worker with pid: 14\n",
      "5au3s64jny-algo-1-izfv3  | [2023-12-28 23:35:56 +0000] [15] [INFO] Booting worker with pid: 15\n",
      "5au3s64jny-algo-1-izfv3  | [2023-12-28 23:35:56 +0000] [23] [INFO] Booting worker with pid: 23\n",
      "5au3s64jny-algo-1-izfv3  | [2023-12-28 23:35:56 +0000] [24] [INFO] Booting worker with pid: 24\n",
      "5au3s64jny-algo-1-izfv3  | [2023-12-28 23:35:56 +0000] [25] [INFO] Booting worker with pid: 25\n",
      "5au3s64jny-algo-1-izfv3  | [2023-12-28 23:35:56 +0000] [33] [INFO] Booting worker with pid: 33\n",
      "5au3s64jny-algo-1-izfv3  | 172.18.0.1 - - [28/Dec/2023:23:36:00 +0000] \"GET /ping HTTP/1.1\" 200 1 \"-\" \"python-urllib3/1.26.18\"\n",
      "!"
     ]
    }
   ],
   "source": [
    "local_predictor = sagemaker_local_model.deploy(\n",
    "                                initial_instance_count= 1,\n",
    "                                instance_type= 'local')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get a prediction locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5au3s64jny-algo-1-izfv3  | result:\n",
      "5au3s64jny-algo-1-izfv3  | {'SayHelloWorldResults': 'Hello, World! Hello, World! Hello, World! '}\n",
      "5au3s64jny-algo-1-izfv3  | 172.18.0.1 - - [28/Dec/2023:23:36:30 +0000] \"POST /invocations HTTP/1.1\" 200 70 \"-\" \"python-urllib3/1.26.18\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'SayHelloWorldResults': 'Hello, World! Hello, World! Hello, World! '}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_json = {\n",
    "    \n",
    "\"SayHelloWorld\": 3\n",
    "    \n",
    "}\n",
    "\n",
    "result = local_predictor.predict(input_json)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deploy to a SageMaker Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----!"
     ]
    }
   ],
   "source": [
    "sagemaker_model = Model(\n",
    "                    sagemaker_session= sagemaker_session,\n",
    "                    model_data = \"s3://\"+bucket+\"/\"+prefix+\"/model.tar.gz\" , \n",
    "                    image_uri= image,\n",
    "                    role=role,\n",
    "                    predictor_cls=PredictorRealTime)\n",
    "\n",
    "predictor = sagemaker_model.deploy(\n",
    "                                initial_instance_count= 1,\n",
    "                                instance_type= 'ml.t2.medium')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get a prediction from our Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'SayHelloWorldResults': 'Hello, World! Hello, World! Hello, World! Hello, World! Hello, World! Hello, World! Hello, World! Hello, World! Hello, World! Hello, World! Hello, World! Hello, World! Hello, World! Hello, World! Hello, World! Hello, World! Hello, World! Hello, World! Hello, World! Hello, World! Hello, World! Hello, World! Hello, World! Hello, World! Hello, World! Hello, World! Hello, World! Hello, World! Hello, World! Hello, World! Hello, World! Hello, World! Hello, World! Hello, World! Hello, World! Hello, World! Hello, World! Hello, World! Hello, World! Hello, World! Hello, World! Hello, World! Hello, World! Hello, World! Hello, World! Hello, World! Hello, World! Hello, World! Hello, World! Hello, World! Hello, World! Hello, World! Hello, World! Hello, World! Hello, World! Hello, World! Hello, World! Hello, World! Hello, World! Hello, World! Hello, World! Hello, World! Hello, World! Hello, World! Hello, World! Hello, World! Hello, World! Hello, World! Hello, World! Hello, World! Hello, World! Hello, World! Hello, World! Hello, World! Hello, World! Hello, World! Hello, World! Hello, World! Hello, World! Hello, World! Hello, World! Hello, World! Hello, World! Hello, World! Hello, World! Hello, World! Hello, World! Hello, World! Hello, World! Hello, World! Hello, World! Hello, World! Hello, World! Hello, World! Hello, World! Hello, World! Hello, World! Hello, World! Hello, World! Hello, World! '}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_json = {\n",
    "    \n",
    "\"SayHelloWorld\": 100\n",
    "    \n",
    "}\n",
    "\n",
    "result = predictor.predict(input_json)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optional cleanup\n",
    "When you're done with the endpoint, you'll want to clean it up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_local_session.delete_endpoint(local_predictor.endpoint)\n",
    "sagemaker_session.delete_endpoint(predictor.endpoint)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
